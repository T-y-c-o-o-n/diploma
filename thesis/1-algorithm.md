# Глава 1. Алгоритм в Базе Данных ВК

## 1.1. Viewstamped Replication

За основу алгоритма разработчиками была взята статья про Viewstamped Replication. Авторы статьи предлагают отказоустойчивый алгоритм в том смысле, что он будет корректно работать, даже когда упадут не более `f` машин, если всего есть `2 * f + 1` реплик.

### 1.1.1. Хранимое состояние на репликах

На репликах хранится топология кластера (упорядоченный список всех нод), номер данной реплики, текущий номер view (+- номер последнего голосования), текущий статус реплики (Normal, ViewChanging или Recovering), журнал лога и commit number. Это всё, что важно в данной работе. Порядок реплик в списке важен и должен быть одинаков на всех - именно в этом порядке выбираются лидеры в голосованиях. Например, в случае 3 реплик, первая будет лидером во всех view с номерами 0, 3, 6, вторая - в 1, 4, 7 и так далее. Это даёт преимущество над алгоритмами с выбором лидера по числу голосов, поскольку, во-первых, в нашем случае лидер каждый раз определённый и не приходится отправлять сообщения с голосами, а во-вторых, при выборах может никто и не набрать кворум голосов. Однако в предопределённом порядке лидеров есть и минус, если новый мастер изолирован по сети от кластера или если он упал. Журнал лога должен храниться в энергонезависимой памяти, чтобы не теряться в случае отказа узла.

### 1.1.2. Normal Operation Protocol

Когда реплика, считающая себя лидером, получает запрос от клиента, она добавляет его в конец лога и отсылает всем репликам сообщение типа Prepare. Нода, получившая актуальный Prepare, тоже добавляет запрос в лог и отсылает лидеру в ответ PrepareOk. Актуальный для неё тот, у которого такой же номер view, как и у неё самой, а так же OpNumber которого (позиция в логе) на 1 больше длины журнала лога машины.

Как только лидер набирает кворум ответов (`f + 1`), включая себя, он считает операцию закоммиченной и увеличивает commit number. Поскольку в данный момент более, чем на половине реплик есть клиентский запрос, то он никуда не потеряется, даже если сеть отрежет текущего мастера или часть реплик, или если мастер упадёт. Следующий лидер точно получит от кого-то этот запрос.

Так же мастер периодически отсылает репликам Commit сообщение для того, чтобы все могли закоммитить и исполнить операцию, а так же, чтобы ноды узнавали, что мастер жив и не запускали новое голосование.

### 1.1.3. View Change Protocol

Если же у кого из реплик сработал таймер на голосование, то она переходит в статус ViewChange, увеличивает view и отсылает всем сообщение StartViewChange. Другая реплика может получить это сообщение и сделать то же самое. Итого все ноды, перходящие в фазу смены лидера, отсылают всем StartViewChange. Делают они это по собственному таймеру или сообщению от другой реплики с номером view, большим, чем у неё.

Как только нода набирает `f` сообщений StartViewChange от других, то это означает, что минимум кворум реплик перешёл в новый view, и старый лидер точно не будет коммитить новые запросы, так как реплика отвечает только на Prepare с её номером view. Сразу после достижения нужного количества сообщений она отсылает *primary* реплике в этом view сообщение DoViewChange, в котором записывает свою информацию: её **полный** лог, commit number, last normal view (Максимальный номер view, в котором нода была в статусе Normal). Это нужно для того, что будущий мастер выбрал лучшую цепочку и скачал её себе.

Далее, как только будущий лидер (но ещё не подтверждённый) получает `f` DoViewChange сообщений без учёта себя, то считает, что кворум реплик знает про новые выборы. Он выбирает реплику, лог которой возьмёт себе и разошлёт другим. Вначале выбирается реплика с максимальным last normal view. Дальше, если таких несколько, то из них берётся с большей длиной лога. Мастер копирует полностью этот лог себе. Потом отсылает всем сообщение StartView со скопированным логом и commit number.

Когда нода в статусе ViewChange получает StartView с таким же номером view или же в любом статусе сообщение c view выше своего - то ставит свой view number равный view из сообщения и статус Normal. Теперь реплика отвечает новому мастеру PrepareOk сообщениями на старые незакомиченные блоки, чтобы тот мог закомиттить то, что не успели старые лидеры. А после этого нода работает по обычному Normal протоколу.

### 1.1.4. Recovery Protocol



## 1.2. Оптимизации в Базе Даных ВК

Разработчики движков в VK реализовали алгоритм из статьи про Viewstamped Replication и сделали некоторые изменения и собственные оптимизации.

### 1.2.1. View блоки в логе

Были добавлены специальные View мета блоки в журнал лога. Каждый такой элемент в истории операций свидетельствует о том, что в данный момент реплика завершила View Change протокол, перешла к новому view и получила Normal статус. View блок с текущим view number коммитит лидер сразу после установки себе лога из лучшей цепочки и прямо перед отправкой StartView сообщений. На этот блок другие реплики так же отвечают PrepareOk, как и на обычные, то есть он реплицируется по обычным правилам, как и клиентские запросы. В этом есть некая унификация - мы реплицируем начало нового view так же, как команды к движкам. Если на кворум реплик этот блок не попал перед началом новой смены view, то может быть такое, что новый лидер не увидит его среди цепочек в сообщениях DoViewChange - то же самое возможно с обычным запросом клиента, который лидер не успел реплицировать на более половины машин. А если кворум успел скачать этот блок - то в будущем он не потеряется точно так же, как и команда к движку.

Помимо этого эти блоки используются в качестве last normal view, нужного в DoViewChange сообщениях - это значение ставится равному view последнего такого блока в журнале.

### 1.2.2. Download Protocol

Передача всего лога в DoViewChange и StartView сообщениях на практике, естественно, не происходит. Вместо этого разработан собственный протокол скачивания, Download Protocol. Реплика, которая хочется скачать лог до какого-то момента с другой посылает той сообщение StartDownload с некоторой информацией об имеющемся у себя журнале. Другая машина сравнивает эту информацию со своей, чтобы определить, есть ли расхождения в журналах - несовпадающие элементы на одинаковых позициях. Потом начинает отсылать обратно сообщения DownloadChunk, в которых передаёт куски лога. Первая нода добавляет себе недостающие элементы журнала, либо заменяет несовпадающие. Это происходит и после того, как новоизбранный мастер выбрал реплику, цепочку которой возьмёт себе в новый view, и после получения нодой сообщения StartView.

Если у реплики, которая в данный момент скачивает что-то с другой, сработает таймер на начало нового view, то скачивание остановится - оно продолжится в будущем с другой реплики по протоколу View Change.	

Так же DownloadChunk сообщения используются в Normal Operation протоколе вместо Prepare - при добавлении нового клиентского запроса мастер броадкастит (рассылает всем репликам) DownloadChunk с новым элементом лога. Это унифицирует то, как одни реплики копируют журнал с других, так как это всегда происходит через сообщения одного вида.

## Выводы по главе 1

В данной главе описан алгоритм из статьи про Viewstamped Replication и его реализация в сервисе репликации движков в ВК.
